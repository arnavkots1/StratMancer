{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Match Data Validation Notebook\n",
        "\n",
        "This notebook validates the schema consistency of collected match data.\n",
        "It samples 100 matches per rank and checks for:\n",
        "- Schema compliance\n",
        "- Data completeness\n",
        "- Value ranges\n",
        "- Statistical summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pydantic import ValidationError\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.append('..')\n",
        "\n",
        "from src.storage.data_storage import DataStorage\n",
        "from src.transformers.schema import MatchData, validate_match\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize storage\n",
        "storage = DataStorage(base_path='../data')\n",
        "\n",
        "# Get statistics\n",
        "stats = storage.get_statistics()\n",
        "print(\"Data Statistics:\")\n",
        "print(f\"Total matches: {stats['total_matches']}\")\n",
        "print(f\"\\nMatches by rank:\")\n",
        "for rank, count in stats['by_rank'].items():\n",
        "    print(f\"  {rank}: {count}\")\n",
        "print(f\"\\nMatches by patch:\")\n",
        "for patch, count in stats['by_patch'].items():\n",
        "    print(f\"  {patch}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Schema Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_sample(rank: str, sample_size: int = 100):\n",
        "    \"\"\"\n",
        "    Validate a sample of matches for a specific rank.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Validating {rank} ===\")\n",
        "    \n",
        "    try:\n",
        "        # Load matches\n",
        "        matches = storage.load_matches(rank, format='parquet')\n",
        "        \n",
        "        if not matches:\n",
        "            print(f\"No matches found for {rank}\")\n",
        "            return None\n",
        "        \n",
        "        # Sample\n",
        "        sample = matches[:min(sample_size, len(matches))]\n",
        "        print(f\"Loaded {len(sample)} matches for validation\")\n",
        "        \n",
        "        # Validate each match\n",
        "        valid_count = 0\n",
        "        errors = []\n",
        "        \n",
        "        for match in sample:\n",
        "            try:\n",
        "                # Already validated during load, but double-check\n",
        "                assert isinstance(match, MatchData)\n",
        "                valid_count += 1\n",
        "            except Exception as e:\n",
        "                errors.append(str(e))\n",
        "        \n",
        "        print(f\"Valid matches: {valid_count}/{len(sample)}\")\n",
        "        \n",
        "        if errors:\n",
        "            print(f\"Errors found: {len(errors)}\")\n",
        "            for i, error in enumerate(errors[:5]):  # Show first 5\n",
        "                print(f\"  {i+1}. {error}\")\n",
        "        \n",
        "        return sample\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Failed to validate {rank}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Validate all ranks\n",
        "ranks = ['IRON', 'BRONZE', 'SILVER', 'GOLD', 'PLATINUM', 'DIAMOND', 'MASTER', 'GRANDMASTER', 'CHALLENGER']\n",
        "validation_results = {}\n",
        "\n",
        "for rank in ranks:\n",
        "    sample = validate_sample(rank, sample_size=100)\n",
        "    if sample:\n",
        "        validation_results[rank] = sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Completeness Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_completeness(matches):\n",
        "    \"\"\"\n",
        "    Check for missing or invalid data.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    \n",
        "    for i, match in enumerate(matches):\n",
        "        # Check picks are not zero\n",
        "        if 0 in match.blue_picks or 0 in match.red_picks:\n",
        "            issues.append(f\"Match {i}: Zero champion in picks\")\n",
        "        \n",
        "        # Check we have 10 champion stats\n",
        "        if len(match.champion_stats) != 10:\n",
        "            issues.append(f\"Match {i}: Expected 10 champion stats, got {len(match.champion_stats)}\")\n",
        "        \n",
        "        # Check shares sum to ~1.0 per team\n",
        "        blue_stats = match.champion_stats[:5]\n",
        "        red_stats = match.champion_stats[5:]\n",
        "        \n",
        "        blue_dmg_share = sum(s.dmg_share for s in blue_stats)\n",
        "        blue_gold_share = sum(s.gold_share for s in blue_stats)\n",
        "        \n",
        "        if not (0.95 <= blue_dmg_share <= 1.05):\n",
        "            issues.append(f\"Match {i}: Blue damage share = {blue_dmg_share:.2f} (expected ~1.0)\")\n",
        "        \n",
        "        if not (0.95 <= blue_gold_share <= 1.05):\n",
        "            issues.append(f\"Match {i}: Blue gold share = {blue_gold_share:.2f} (expected ~1.0)\")\n",
        "    \n",
        "    return issues\n",
        "\n",
        "print(\"\\n=== Completeness Check ===\")\n",
        "for rank, matches in validation_results.items():\n",
        "    issues = check_completeness(matches)\n",
        "    print(f\"\\n{rank}: {len(issues)} issues found\")\n",
        "    if issues:\n",
        "        for issue in issues[:5]:  # Show first 5\n",
        "            print(f\"  - {issue}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Statistical Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_statistics(matches):\n",
        "    \"\"\"\n",
        "    Compute statistical summaries.\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'blue_win_rate': sum(1 for m in matches if m.blue_win) / len(matches),\n",
        "        'avg_dragons': np.mean([m.blue_objectives.dragons + m.red_objectives.dragons for m in matches]),\n",
        "        'avg_barons': np.mean([m.blue_objectives.barons + m.red_objectives.barons for m in matches]),\n",
        "        'avg_towers': np.mean([m.blue_objectives.towers + m.red_objectives.towers for m in matches]),\n",
        "        'avg_kda': np.mean([s.kda for m in matches for s in m.champion_stats]),\n",
        "        'avg_cs': np.mean([s.cs for m in matches for s in m.champion_stats]),\n",
        "        'avg_ap_ad_ratio': np.mean([m.derived_features.ap_ad_ratio for m in matches]),\n",
        "        'avg_engage_score': np.mean([m.derived_features.engage_score for m in matches]),\n",
        "        'avg_teamfight_synergy': np.mean([m.derived_features.teamfight_synergy for m in matches]),\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "print(\"\\n=== Statistical Summary ===\")\n",
        "summary_df_data = []\n",
        "\n",
        "for rank, matches in validation_results.items():\n",
        "    stats = compute_statistics(matches)\n",
        "    stats['rank'] = rank\n",
        "    stats['sample_size'] = len(matches)\n",
        "    summary_df_data.append(stats)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_df_data)\n",
        "summary_df = summary_df[['rank', 'sample_size', 'blue_win_rate', 'avg_dragons', 'avg_barons', \n",
        "                         'avg_towers', 'avg_kda', 'avg_cs', 'avg_ap_ad_ratio', \n",
        "                         'avg_engage_score', 'avg_teamfight_synergy']]\n",
        "\n",
        "print(summary_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Validation Complete\n",
        "\n",
        "The dataset is validated and ready for ML training!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
